{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc23080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 18:30:10.845446: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-16 18:30:10.847373: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-16 18:30:10.881210: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-16 18:30:10.881863: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-16 18:30:11.404306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.insert(0, f\"{os.path.dirname(os.getcwd())}/src\")\n",
    "from utils import evaluate_result\n",
    "from data_imbalance_src.smote_oversampling import RandomOversampling, ADASYNOversampling, BorderlineSMOTEOversampling, SMOTEOversampling, SVMSMOTEOversampling\n",
    "from data_imbalance_src.smote_oversampling import SMOTUNEDOversampling\n",
    "from data_imbalance_src.dazzle import DAZZLEOversampling\n",
    "from data_imbalance_src.Imbalance_Farou2022.data_generation import GANOversampling\n",
    "from data_imbalance_src.random_projection import RandomProjectionOversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2279378",
   "metadata": {},
   "source": [
    "# JavaScript_Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dddba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file = \"JS_Vuln_res_r8.csv\"\n",
    "write_path = f\"{os.path.dirname(os.getcwd())}/result/{write_file}\"\n",
    "with open(write_path, \"w\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([\"oversampling_scheme\", \"runtime\", \"learner\", \"acc\", \"prec\", \"recall\", \"fpr\", \"f1\", \"auc\", \"g_score\", \"d2h\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df5773e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JSVulnerabilityDataSet-1.0.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = f\"{os.path.dirname(os.getcwd())}/data/JavaScript_Vulnerability/\"\n",
    "datafiles = [f for f in os.listdir(data_path) if f.endswith(\"csv\")]\n",
    "datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d1bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{data_path}/{datafiles[0]}\")\n",
    "drop_columns = [\"name\", \"longname\", \"path\", \"full_repo_path\", \"line\", \"column\", \"endline\", \"endcolumn\"]\n",
    "df = df.drop(drop_columns, axis=1)\n",
    "df = df.drop_duplicates()\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc09bb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y value counts: \n",
      " 0    5367\n",
      "1     904\n",
      "Name: Vuln, dtype: int64\n",
      "y class ratio: 1: 6\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "print(\"y value counts: \\n\", str(y.value_counts()))\n",
    "print(\"y class ratio: 1:\", str(round(y.value_counts()[0]/y.value_counts()[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4055c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- y train classes count: \n",
      "0    4293\n",
      "1     723\n",
      "Name: Vuln, dtype: int64\n",
      "--- y train ratio: 1:6\n",
      " \n",
      "--- y test classes count: \n",
      "0    1074\n",
      "1     181\n",
      "Name: Vuln, dtype: int64\n",
      "--- y test ratio: 1:6\n"
     ]
    }
   ],
   "source": [
    "rs = random.randint(0, 100000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=rs)\n",
    "print(\"--- y train classes count: \\n\" + str(y_train.value_counts()))\n",
    "print(\"--- y train ratio: 1:\" + str(round(y_train.value_counts()[0] / y_train.value_counts()[1])))\n",
    "print(\" \")\n",
    "print(\"--- y test classes count: \\n\" + str(y_test.value_counts()))\n",
    "print(\"--- y test ratio: 1:\" + str(round(y_test.value_counts()[0] / y_test.value_counts()[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2882950",
   "metadata": {},
   "source": [
    "### Normal Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a551b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal run - without any oversampling technique\n",
    "# inputs: X_train, y_train, X_test, y_test\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_scale, y_train)\n",
    "clf_KNN.fit(X_train_scale, y_train)\n",
    "clf_LR.fit(X_train_scale, y_train)\n",
    "clf_DT.fit(X_train_scale, y_train)\n",
    "clf_RF.fit(X_train_scale, y_train)\n",
    "clf_LightGBM.fit(X_train_scale, y_train)\n",
    "clf_Adaboost.fit(X_train_scale, y_train)\n",
    "clf_GBDT.fit(X_train_scale, y_train)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test_scale)\n",
    "y_pred_KNN = clf_KNN.predict(X_test_scale)\n",
    "y_pred_LR = clf_LR.predict(X_test_scale)\n",
    "y_pred_DT = clf_DT.predict(X_test_scale)\n",
    "y_pred_RF = clf_RF.predict(X_test_scale)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test_scale)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test_scale)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef74cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"No\", 0, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"No\", 0, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"No\", 0, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"No\", 0, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"No\", 0, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"No\", 0, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"No\", 0, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"No\", 0, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cfd37",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f33936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# random oversampling run - random oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "rt, X_train_new, y_train_new = RandomOversampling(X_train=X_train, y_train=y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d0c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"Random\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"Random\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"Random\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"Random\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"Random\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"Random\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"Random\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"Random\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281db9c2",
   "metadata": {},
   "source": [
    "### ADASYN Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfbda71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# ADASYN oversampling run - ADASYN oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "rt, X_train_new, y_train_new = ADASYNOversampling(X_train=X_train, y_train=y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69242434",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"ADASYN\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"ADASYN\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"ADASYN\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"ADASYN\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"ADASYN\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"ADASYN\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"ADASYN\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"ADASYN\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58555c",
   "metadata": {},
   "source": [
    "### BorderlineSMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be4efbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# BorderlineSMOTE oversampling run - BorderlineSMOTE oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "rt, X_train_new, y_train_new = BorderlineSMOTEOversampling(X_train=X_train, y_train=y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4eb176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"BorderlineSMOTE\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"BorderlineSMOTE\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"BorderlineSMOTE\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"BorderlineSMOTE\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"BorderlineSMOTE\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"BorderlineSMOTE\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"BorderlineSMOTE\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"BorderlineSMOTE\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10af448",
   "metadata": {},
   "source": [
    "### SMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f35552fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# SMOTE oversampling run - SMOTE oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "rt, X_train_new, y_train_new = SMOTEOversampling(X_train=X_train, y_train=y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "861996c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"SMOTE\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"SMOTE\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"SMOTE\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"SMOTE\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"SMOTE\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"SMOTE\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"SMOTE\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"SMOTE\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030725b0",
   "metadata": {},
   "source": [
    "### SVMSMOTE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a8f0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# SVMSMOTE oversampling run - SVMSMOTE oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "rt, X_train_new, y_train_new = SVMSMOTEOversampling(X_train=X_train, y_train=y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38e7465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"SVMSMOTE\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"SVMSMOTE\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"SVMSMOTE\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"SVMSMOTE\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"SVMSMOTE\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"SVMSMOTE\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"SVMSMOTE\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"SVMSMOTE\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4dca6",
   "metadata": {},
   "source": [
    "### SMOTUNED Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45eadca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio of SVM: 1:1\n",
      "y train ratio of KNN: 1:1\n",
      "y train ratio of LR: 1:1\n",
      "y train ratio of DT: 1:1\n",
      "y train ratio of RF: 1:1\n",
      "y train ratio of LightGBM: 1:1\n",
      "y train ratio of Adaboost: 1:1\n",
      "y train ratio of GBDT: 1:1\n"
     ]
    }
   ],
   "source": [
    "# SMOTUNED oversampling run - SMOTUNED oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "rt_SVM, X_train_new_SVM, y_train_new_SVM = SMOTUNEDOversampling(X_train=X_train, X_test=X_test, \n",
    "                                                                y_train=y_train, y_test=y_test, model=\"SVM\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new_SVM), columns=X_train_new_SVM.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"y train ratio of SVM: 1:\" + str(round(y_train_new_SVM.value_counts()[0] / y_train_new_SVM.value_counts()[1])))\n",
    "\n",
    "rt_KNN, X_train_new_KNN, y_train_new_KNN = SMOTUNEDOversampling(X_train=X_train, X_test=X_test, \n",
    "                                                                y_train=y_train, y_test=y_test, model=\"KNN\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new_KNN), columns=X_train_new_KNN.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"y train ratio of KNN: 1:\" + str(round(y_train_new_KNN.value_counts()[0] / y_train_new_KNN.value_counts()[1])))\n",
    "\n",
    "rt_LR, X_train_new_LR, y_train_new_LR = SMOTUNEDOversampling(X_train=X_train, X_test=X_test, \n",
    "                                                             y_train=y_train, y_test=y_test, model=\"LR\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new_LR), columns=X_train_new_LR.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"y train ratio of LR: 1:\" + str(round(y_train_new_LR.value_counts()[0] / y_train_new_LR.value_counts()[1])))\n",
    "\n",
    "rt_DT, X_train_new_DT, y_train_new_DT = SMOTUNEDOversampling(X_train=X_train, X_test=X_test, \n",
    "                                                             y_train=y_train, y_test=y_test, model=\"DT\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new_DT), columns=X_train_new_DT.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"y train ratio of DT: 1:\" + str(round(y_train_new_DT.value_counts()[0] / y_train_new_DT.value_counts()[1])))\n",
    "\n",
    "rt_RF, X_train_new_RF, y_train_new_RF = SMOTUNEDOversampling(X_train=X_train, X_test=X_test, \n",
    "                                                             y_train=y_train, y_test=y_test, model=\"RF\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new_RF), columns=X_train_new_RF.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"y train ratio of RF: 1:\" + str(round(y_train_new_RF.value_counts()[0] / y_train_new_RF.value_counts()[1])))\n",
    "\n",
    "rt_LightGBM, X_train_new_LightGBM, y_train_new_LightGBM = SMOTUNEDOversampling(X_train=X_train, X_test=X_test, \n",
    "                                                                               y_train=y_train, y_test=y_test, model=\"LightGBM\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new_LightGBM), columns=X_train_new_LightGBM.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"y train ratio of LightGBM: 1:\" + str(round(y_train_new_LightGBM.value_counts()[0] / y_train_new_LightGBM.value_counts()[1])))\n",
    "\n",
    "rt_Adaboost, X_train_new_Adaboost, y_train_new_Adaboost = SMOTUNEDOversampling(X_train=X_train, X_test=X_test, \n",
    "                                                                               y_train=y_train, y_test=y_test, model=\"Adaboost\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new_Adaboost), columns=X_train_new_Adaboost.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"y train ratio of Adaboost: 1:\" + str(round(y_train_new_Adaboost.value_counts()[0] / y_train_new_Adaboost.value_counts()[1])))\n",
    "\n",
    "rt_GBDT, X_train_new_GBDT, y_train_new_GBDT = SMOTUNEDOversampling(X_train=X_train, X_test=X_test, \n",
    "                                                                   y_train=y_train, y_test=y_test, model=\"GBDT\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new_GBDT), columns=X_train_new_GBDT.columns)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(\"y train ratio of GBDT: 1:\" + str(round(y_train_new_GBDT.value_counts()[0] / y_train_new_GBDT.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new_SVM, y_train_new_SVM)\n",
    "clf_KNN.fit(X_train_new_KNN, y_train_new_KNN)\n",
    "clf_LR.fit(X_train_new_LR, y_train_new_LR)\n",
    "clf_DT.fit(X_train_new_DT, y_train_new_DT)\n",
    "clf_RF.fit(X_train_new_RF, y_train_new_RF)\n",
    "clf_LightGBM.fit(X_train_new_LightGBM, y_train_new_LightGBM)\n",
    "clf_Adaboost.fit(X_train_new_Adaboost, y_train_new_Adaboost)\n",
    "clf_GBDT.fit(X_train_new_GBDT, y_train_new_GBDT)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdd1a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"SMOTUNED\", rt_SVM, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"SMOTUNED\", rt_KNN, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"SMOTUNED\", rt_LR, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"SMOTUNED\", rt_DT, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"SMOTUNED\", rt_RF, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"SMOTUNED\", rt_LightGBM, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"SMOTUNED\", rt_Adaboost, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"SMOTUNED\", rt_GBDT, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4ab51",
   "metadata": {},
   "source": [
    "### DAZZLE Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eb04177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                           | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 19:02:23.865026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-16 19:02:23.865398: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [01:01<00:00,  1.61trial/s, best loss: -0.9072813009735773]\n",
      "Best Hyperparameters: {'batch_size': 128, 'discriminator_activation_fn': <function tanh at 0x7f35b4a04ee0>, 'discriminator_layer_normalization': True, 'discriminator_lr': 0.06048168094334402, 'discriminator_optimizer': <class 'keras.src.optimizers.rmsprop.RMSprop'>, 'epochs': 15, 'generator_activation_fn': <function sigmoid at 0x7f36581a8790>, 'generator_layer_normalization': True, 'generator_lr': 0.0009761039789139196, 'generator_optimizer': <class 'keras.src.optimizers.adamax.Adamax'>}\n",
      "Best G-Measure: 0.9072813009735773\n",
      " 26/157 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step\n",
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# DAZZLE oversampling run - DAZZLE oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_GAN = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "rt, X_train_new, y_train_new = DAZZLEOversampling(X_train=X_train_GAN, y_train=y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23be0d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"DAZZLE\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"DAZZLE\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"DAZZLE\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"DAZZLE\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"DAZZLE\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"DAZZLE\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"DAZZLE\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"DAZZLE\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ee189",
   "metadata": {},
   "source": [
    "### WGAN Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "889973b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX CLASS 4293\n",
      "CLASS ID 1\n",
      "Epoch 1/150 completed. Gen loss: 0.025218738242983818. Desc loss_real: -0.004391619469970465. Desc loss_fake: -0.025218738242983818\n",
      "Epoch 51/150 completed. Gen loss: -0.0020170125644654036. Desc loss_real: 0.0043651266023516655. Desc loss_fake: 0.0020170125644654036\n",
      "Epoch 101/150 completed. Gen loss: -0.0008252797997556627. Desc loss_real: 0.0023122106213122606. Desc loss_fake: 0.0008252797997556627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/Research/SyntheticData/src/data_imbalance_src/Imbalance_Farou2022/data_generation.py:119: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_data = new_data.append(synthetic_data)\n",
      "/mnt/e/Research/SyntheticData/src/data_imbalance_src/Imbalance_Farou2022/data_generation.py:133: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_sample = X_sample.append(X_train)\n",
      "/mnt/e/Research/SyntheticData/src/data_imbalance_src/Imbalance_Farou2022/data_generation.py:135: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_sample = X_sample.drop(tar, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# WGAN oversampling run - WGAN oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_GAN = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "rt, X_train_new, y_train_new = GANOversampling(X_train=X_train_GAN, y_train=y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0a87e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"WGAN\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"WGAN\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"WGAN\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"WGAN\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"WGAN\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"WGAN\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"WGAN\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"WGAN\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e3384",
   "metadata": {},
   "source": [
    "### Random Projection Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9875370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# Random projection oversampling run - Random projection oversampling technique\n",
    "# inputs: X_train_random, y_train_random, X_test, y_test\n",
    "\n",
    "rt, X_train_new, y_train_new = RandomProjectionOversampling(X_train=X_train, y_train=y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "398fad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"RP\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"RP\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"RP\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"RP\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"RP\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"RP\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"RP\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"RP\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6625ff",
   "metadata": {},
   "source": [
    "### Diveplane Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "288bec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diveplane.utilities import infer_feature_attributes\n",
    "from diveplane.geminai import Geminai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7908f824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:diveplane.local.client:Version 2.1.185 of Diveplane® Local is available. You are using version 1.0.298.\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/diveplane/geminai/base.py:376: UserWarning: The value of the parameter `generate_new_cases` is set as 'no' and not 'always', which means it is possible that some cases that are very similar to the original data may be returned. To prevent this from happening, please set generate_new_cases to 'always'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tar = y_train.name\n",
    "conditions = [{tar: 1},\n",
    "              {tar: 0}] * (int(X_train.shape[0] / 2))\n",
    "\n",
    "X_train[tar] = y_train\n",
    "partial_features = {\"CLLC\": {'type': \"continuous\"}}\n",
    "features = infer_feature_attributes(X_train, features=partial_features)\n",
    "for f_name, f_value in features.items():\n",
    "    if f_value[\"type\"] == \"nominal\":\n",
    "        f_value[\"non_sensitive\"] = True\n",
    "\n",
    "start_time = time.time()\n",
    "g = Geminai()\n",
    "g.train(X_train, features=features)\n",
    "\n",
    "gen_df = g.synthesize_cases(\n",
    "    n_samples=len(conditions),\n",
    "    case_context_values_maps=conditions,\n",
    "    generate_new_cases=\"no\"\n",
    ")\n",
    "\n",
    "rt = time.time() - start_time\n",
    "\n",
    "X_train = X_train.iloc[:, :-1]\n",
    "X_train_new = gen_df.iloc[:, :-1]\n",
    "y_train_new = gen_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daf865a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82a127b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"Diveplane\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"Diveplane\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"Diveplane\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"Diveplane\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"Diveplane\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"Diveplane\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"Diveplane\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"Diveplane\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18849a01",
   "metadata": {},
   "source": [
    "### DS Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4f2eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSynthesizer.DataDescriber import DataDescriber\n",
    "from DataSynthesizer.DataGenerator import DataGenerator\n",
    "from DataSynthesizer.ModelInspector import ModelInspector\n",
    "from DataSynthesizer.lib.utils import read_json_file, display_bayesian_network\n",
    "mode = \"independent_attribute_mode\"\n",
    "\n",
    "col = X_train.columns\n",
    "tar = y_train.name\n",
    "X_train[tar] = y_train\n",
    "write_df = X_train[X_train[tar] == 1]\n",
    "write_df = write_df.iloc[:, :-1]\n",
    "write_df.to_csv(f\"{os.path.dirname(os.getcwd())}/extra/js_vuln_pos_df.csv\", index=False)\n",
    "X_train = X_train.iloc[:, :-1]\n",
    "\n",
    "threshold = 20\n",
    "num_tuples_to_generate = int(y_train.value_counts()[0] - y_train.value_counts()[1])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "description_file = f\"{os.path.dirname(os.getcwd())}/extra/js_vuln.json\"\n",
    "describer = DataDescriber(category_threshold=threshold)\n",
    "describer.describe_dataset_in_independent_attribute_mode(\n",
    "    dataset_file=f\"{os.path.dirname(os.getcwd())}/extra/js_vuln_pos_df.csv\"\n",
    ")\n",
    "describer.save_dataset_description_to_file(description_file)\n",
    "\n",
    "generator = DataGenerator()\n",
    "generator.generate_dataset_in_independent_mode(num_tuples_to_generate, description_file)\n",
    "generator.save_synthetic_data(f\"{os.path.dirname(os.getcwd())}/extra/js_vuln_syn_df.csv\")\n",
    "\n",
    "rt = time.time() - start_time\n",
    "\n",
    "X_train_new = pd.read_csv(f\"{os.path.dirname(os.getcwd())}/extra/js_vuln_syn_df.csv\").to_numpy()\n",
    "y_train_new = np.ones(num_tuples_to_generate)\n",
    "X_train_new = pd.DataFrame(np.vstack((X_train.to_numpy(), X_train_new)), columns=col)\n",
    "y_train_new = pd.Series(np.hstack((y_train.to_numpy(), y_train_new)), name=tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97d2dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "109e7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"DS\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"DS\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"DS\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"DS\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"DS\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"DS\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"DS\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"DS\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40c3d8",
   "metadata": {},
   "source": [
    "### SDV Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25c763b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.lite import SingleTablePreset\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.single_table import GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00cba1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = X_train.columns\n",
    "tar = y_train.name\n",
    "num_tuples_to_generate = int(y_train.value_counts()[0] - y_train.value_counts()[1])\n",
    "X_train[tar] = y_train\n",
    "pos_df = X_train[X_train[tar] == 1]\n",
    "pos_df = pos_df.iloc[:, :-1]\n",
    "X_train = X_train.iloc[:, :-1]\n",
    "\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data=pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea2ce021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "syn1 = SingleTablePreset(metadata, name=\"FAST_ML\")\n",
    "syn1.fit(data=pos_df)\n",
    "X_train_new = syn1.sample(num_rows=num_tuples_to_generate).to_numpy()\n",
    "\n",
    "rt = time.time() - start_time\n",
    "\n",
    "X_train_new = pd.DataFrame(np.vstack((X_train.to_numpy(), X_train_new)), columns=col)\n",
    "y_train_new = np.ones(num_tuples_to_generate)\n",
    "y_train_new = pd.Series(np.hstack((y_train.to_numpy(), y_train_new)), name=tar)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b99674a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"SDV_FASTML\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"SDV_FASTML\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"SDV_FASTML\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"SDV_FASTML\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"SDV_FASTML\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"SDV_FASTML\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"SDV_FASTML\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"SDV_FASTML\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d41d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HDIFF'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HVOL'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HEFF'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HBUGS'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HTIME'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'CYCL_DENS'. Data will not be rounded.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "syn2 = GaussianCopulaSynthesizer(metadata)\n",
    "syn2.fit(data=pos_df)\n",
    "X_train_new = syn2.sample(num_rows=num_tuples_to_generate).to_numpy()\n",
    "\n",
    "rt = time.time() - start_time\n",
    "\n",
    "X_train_new = pd.DataFrame(np.vstack((X_train.to_numpy(), X_train_new)), columns=col)\n",
    "y_train_new = np.ones(num_tuples_to_generate)\n",
    "y_train_new = pd.Series(np.hstack((y_train.to_numpy(), y_train_new)), name=tar)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3003110",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"SDV_GC\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"SDV_GC\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"SDV_GC\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"SDV_GC\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"SDV_GC\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"SDV_GC\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"SDV_GC\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"SDV_GC\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fe59698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HDIFF'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HVOL'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HEFF'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HBUGS'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'HTIME'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/numerical.py:112: UserWarning: No rounding scheme detected for column 'CYCL_DENS'. Data will not be rounded.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n",
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train ratio: 1:1\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "syn3 = CTGANSynthesizer(metadata)\n",
    "syn3.fit(data=pos_df)\n",
    "X_train_new = syn3.sample(num_rows=num_tuples_to_generate).to_numpy()\n",
    "\n",
    "rt = time.time() - start_time\n",
    "\n",
    "X_train_new = pd.DataFrame(np.vstack((X_train.to_numpy(), X_train_new)), columns=col)\n",
    "y_train_new = np.ones(num_tuples_to_generate)\n",
    "y_train_new = pd.Series(np.hstack((y_train.to_numpy(), y_train_new)), name=tar)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = pd.DataFrame(scaler.fit_transform(X_train_new), columns=X_train_new.columns, index=X_train_new.index)\n",
    "X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"y train ratio: 1:\" + str(round(y_train_new.value_counts()[0] / y_train_new.value_counts()[1])))\n",
    "\n",
    "# create models\n",
    "clf_SVM = SVC()\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf_LR = LogisticRegression(random_state=42, solver=\"saga\", max_iter=20000, n_jobs=-1)\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "clf_LightGBM = LGBMClassifier(objective=\"binary\", random_state=42, n_jobs=-1)\n",
    "clf_Adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "clf_GBDT = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "\n",
    "clf_SVM.fit(X_train_new, y_train_new)\n",
    "clf_KNN.fit(X_train_new, y_train_new)\n",
    "clf_LR.fit(X_train_new, y_train_new)\n",
    "clf_DT.fit(X_train_new, y_train_new)\n",
    "clf_RF.fit(X_train_new, y_train_new)\n",
    "clf_LightGBM.fit(X_train_new, y_train_new)\n",
    "clf_Adaboost.fit(X_train_new, y_train_new)\n",
    "clf_GBDT.fit(X_train_new, y_train_new)\n",
    "\n",
    "y_pred_SVM = clf_SVM.predict(X_test)\n",
    "y_pred_KNN = clf_KNN.predict(X_test)\n",
    "y_pred_LR = clf_LR.predict(X_test)\n",
    "y_pred_DT = clf_DT.predict(X_test)\n",
    "y_pred_RF = clf_RF.predict(X_test)\n",
    "y_pred_LightGBM = clf_LightGBM.predict(X_test)\n",
    "y_pred_Adaboost = clf_Adaboost.predict(X_test)\n",
    "y_pred_GBDT = clf_GBDT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91df5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{os.path.dirname(os.getcwd())}/result/{write_file}\", \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    \n",
    "    csv_writer.writerow([\"SDV_GAN\", rt, \"SVM\"] + evaluate_result(y_pred_SVM, y_test))\n",
    "    csv_writer.writerow([\"SDV_GAN\", rt, \"KNN\"] + evaluate_result(y_pred_KNN, y_test))\n",
    "    csv_writer.writerow([\"SDV_GAN\", rt, \"LR\"] + evaluate_result(y_pred_LR, y_test))\n",
    "    csv_writer.writerow([\"SDV_GAN\", rt, \"DT\"] + evaluate_result(y_pred_DT, y_test))\n",
    "    csv_writer.writerow([\"SDV_GAN\", rt, \"RF\"] + evaluate_result(y_pred_RF, y_test))\n",
    "    csv_writer.writerow([\"SDV_GAN\", rt, \"LightGBM\"] + evaluate_result(y_pred_LightGBM, y_test))\n",
    "    csv_writer.writerow([\"SDV_GAN\", rt, \"Adaboost\"] + evaluate_result(y_pred_Adaboost, y_test))\n",
    "    csv_writer.writerow([\"SDV_GAN\", rt, \"GBDT\"] + evaluate_result(y_pred_GBDT, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5faafe7",
   "metadata": {},
   "source": [
    "# Process data and write to txt file for statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3af65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
