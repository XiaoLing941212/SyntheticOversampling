{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzsoGI9bArNF"
   },
   "source": [
    "## Package instalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1a9yX5RgAJ_O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feature_engine\n",
      "  Downloading feature_engine-1.6.1-py2.py3-none-any.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from feature_engine) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.0.3 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from feature_engine) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from feature_engine) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from feature_engine) (1.10.1)\n",
      "Requirement already satisfied: statsmodels>=0.11.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from feature_engine) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from pandas>=1.0.3->feature_engine) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from statsmodels>=0.11.1->feature_engine) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from packaging>=21.3->statsmodels>=0.11.1->feature_engine) (3.0.9)\n",
      "Requirement already satisfied: six in /home/lingxiao/diveplane/lib/python3.8/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.16.0)\n",
      "Installing collected packages: feature_engine\n",
      "Successfully installed feature_engine-1.6.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tabgan\n",
      "  Downloading tabgan-1.3.3-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: pandas in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tabgan) (1.5.3)\n",
      "Requirement already satisfied: numpy in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tabgan) (1.23.5)\n",
      "Collecting category-encoders (from tabgan)\n",
      "  Downloading category_encoders-2.6.2-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tabgan) (2.0.1)\n",
      "Requirement already satisfied: lightgbm>=2.2.3 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tabgan) (3.3.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tabgan) (1.2.2)\n",
      "Collecting torchvision (from tabgan)\n",
      "  Downloading torchvision-0.15.2-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tabgan) (2.8.2)\n",
      "Requirement already satisfied: tqdm in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tabgan) (4.65.0)\n",
      "Requirement already satisfied: wheel in /home/lingxiao/diveplane/lib/python3.8/site-packages (from lightgbm>=2.2.3->tabgan) (0.40.0)\n",
      "Requirement already satisfied: scipy in /home/lingxiao/diveplane/lib/python3.8/site-packages (from lightgbm>=2.2.3->tabgan) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from scikit-learn>=1.0.2->tabgan) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from scikit-learn>=1.0.2->tabgan) (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (1.12)\n",
      "Requirement already satisfied: networkx in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torch>=1.0->tabgan) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/lingxiao/diveplane/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0->tabgan) (56.0.0)\n",
      "Requirement already satisfied: cmake in /home/lingxiao/diveplane/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.0->tabgan) (3.26.4)\n",
      "Requirement already satisfied: lit in /home/lingxiao/diveplane/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.0->tabgan) (16.0.6)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from category-encoders->tabgan) (0.14.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from category-encoders->tabgan) (0.5.3)\n",
      "Requirement already satisfied: importlib-resources in /home/lingxiao/diveplane/lib/python3.8/site-packages (from category-encoders->tabgan) (5.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from pandas->tabgan) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from python-dateutil->tabgan) (1.16.0)\n",
      "Requirement already satisfied: requests in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torchvision->tabgan) (2.30.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from torchvision->tabgan) (9.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging>=21.3 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from statsmodels>=0.9.0->category-encoders->tabgan) (21.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from importlib-resources->category-encoders->tabgan) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from jinja2->torch>=1.0->tabgan) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from requests->torchvision->tabgan) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from requests->torchvision->tabgan) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from requests->torchvision->tabgan) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from requests->torchvision->tabgan) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from sympy->torch>=1.0->tabgan) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from packaging>=21.3->statsmodels>=0.9.0->category-encoders->tabgan) (3.0.9)\n",
      "Installing collected packages: category-encoders, torchvision, tabgan\n",
      "Successfully installed category-encoders-2.6.2 tabgan-1.3.3 torchvision-0.15.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install feature_engine\n",
    "!pip install tabgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hzpskW8cAL-N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting variants\n",
      "  Downloading variants-0.2.0-py2.py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: variants\n",
      "Successfully installed variants-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting smote_variants\n",
      "  Downloading smote_variants-0.7.1-py3-none-any.whl (407 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.4/407.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from smote_variants) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/lingxiao/diveplane/lib/python3.8/site-packages (from smote_variants) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /home/lingxiao/diveplane/lib/python3.8/site-packages (from smote_variants) (1.2.2)\n",
      "Requirement already satisfied: joblib in /home/lingxiao/diveplane/lib/python3.8/site-packages (from smote_variants) (1.2.0)\n",
      "Collecting minisom (from smote_variants)\n",
      "  Downloading MiniSom-2.3.1.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting statistics (from smote_variants)\n",
      "  Downloading statistics-1.0.3.5.tar.gz (8.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /home/lingxiao/diveplane/lib/python3.8/site-packages (from smote_variants) (2.13.0)\n",
      "Requirement already satisfied: keras in /home/lingxiao/diveplane/lib/python3.8/site-packages (from smote_variants) (2.13.1)\n",
      "Requirement already satisfied: pandas in /home/lingxiao/diveplane/lib/python3.8/site-packages (from smote_variants) (1.5.3)\n",
      "Collecting mkl (from smote_variants)\n",
      "  Downloading mkl-2023.2.0-py2.py3-none-manylinux1_x86_64.whl (258.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.0/258.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting metric-learn (from smote_variants)\n",
      "  Downloading metric_learn-0.6.2-py2.py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: seaborn in /home/lingxiao/diveplane/lib/python3.8/site-packages (from smote_variants) (0.12.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from scikit-learn->smote_variants) (3.1.0)\n",
      "Collecting intel-openmp==2023.* (from mkl->smote_variants)\n",
      "  Downloading intel_openmp-2023.2.0-py2.py3-none-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tbb==2021.* (from mkl->smote_variants)\n",
      "  Downloading tbb-2021.10.0-py2.py3-none-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from pandas->smote_variants) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from pandas->smote_variants) (2023.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from seaborn->smote_variants) (3.7.1)\n",
      "Collecting docutils>=0.3 (from statistics->smote_variants)\n",
      "  Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (4.24.2)\n",
      "Requirement already satisfied: setuptools in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (56.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorflow->smote_variants) (0.33.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow->smote_variants) (0.40.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (5.12.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow->smote_variants) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow->smote_variants) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow->smote_variants) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow->smote_variants) (2.30.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow->smote_variants) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow->smote_variants) (2.3.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (1.26.15)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (1.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn->smote_variants) (3.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (6.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/lingxiao/diveplane/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->smote_variants) (3.2.2)\n",
      "Building wheels for collected packages: minisom, statistics\n",
      "  Building wheel for minisom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for minisom: filename=MiniSom-2.3.1-py3-none-any.whl size=10592 sha256=7c599ec5e670294869507f66ce0a27d88fae642598725abb2cd3e7a5ac5e87ad\n",
      "  Stored in directory: /home/lingxiao/.cache/pip/wheels/7b/fd/40/c318df5c7fa3b276930ab30fed7306b603007e9b24d5479958\n",
      "  Building wheel for statistics (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for statistics: filename=statistics-1.0.3.5-py3-none-any.whl size=7454 sha256=3b32872f643883f6aee0ee985a804ef1c00754062168b8184f759e89b4aebb6d\n",
      "  Stored in directory: /home/lingxiao/.cache/pip/wheels/36/4b/c7/6af97584669b756c0d60c5ff05d5fb1f533a4e4d96e5ee92b9\n",
      "Successfully built minisom statistics\n",
      "Installing collected packages: tbb, minisom, intel-openmp, mkl, docutils, statistics, metric-learn, smote_variants\n",
      "Successfully installed docutils-0.20.1 intel-openmp-2023.2.0 metric-learn-0.6.2 minisom-2.3.1 mkl-2023.2.0 smote_variants-0.7.1 statistics-1.0.3.5 tbb-2021.10.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install variants\n",
    "!pip install smote_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfb363dVAxdg"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TmirlPO52IJC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 10:52:38.689896: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-07 10:52:38.698612: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-07 10:52:38.819875: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-07 10:52:38.821483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 10:52:39.570066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_29243/2126172336.py:19: experimental_run_functions_eagerly (from tensorflow.python.eager.polymorphic_function.quarantine) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingxiao/diveplane/lib/python3.8/site-packages/_ctgan/synthesizer.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import time\n",
    "import collections \n",
    "from matplotlib import pyplot as plt\n",
    "import argparse\n",
    "import json\n",
    "import scipy.io as sio\n",
    "from sklearn import metrics, preprocessing\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "import tensorflow as tf \n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.decomposition import PCA\n",
    "import variants as variants\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import smote_variants as sv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tabgan.sampler import OriginalGenerator, GANGenerator\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUALAcBVA8i9"
   },
   "source": [
    "## WGAN and related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fCUZfqz829al"
   },
   "outputs": [],
   "source": [
    "# generator\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self,n_inp,n_noise,n_hid=128):\n",
    "        super().__init__()\n",
    "        init=tf.keras.initializers.GlorotUniform\n",
    "        self.input_layer=Dense(units=n_noise,kernel_initializer=init)\n",
    "        self.hidden_layer=Dense(units=n_hid,activation=\"relu\",kernel_initializer=init)\n",
    "        self.output_layer=Dense(units=n_inp,activation=\"sigmoid\",kernel_initializer=init)\n",
    "    def call(self,inputs):\n",
    "        x=self.input_layer(inputs)\n",
    "        x=self.hidden_layer(x)\n",
    "        return self.output_layer(x)\n",
    "# critic   \n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,n_inp,n_hid=128):\n",
    "        super().__init__()\n",
    "        init=tf.keras.initializers.GlorotUniform\n",
    "        self.input_layer=Dense(units=n_inp,kernel_initializer=init)\n",
    "        self.hidden_layer=Dense(units=n_hid,activation=\"relu\",kernel_initializer=init)\n",
    "        self.logits=Dense(units=1,activation=None,kernel_initializer=init)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x=self.input_layer(inputs)\n",
    "        x=self.hidden_layer(x)\n",
    "        return self.logits(x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_data,gen,critic,noise_dim,generator_optimizer,critic_optimizer):\n",
    "    batch_size=real_data.shape[0]# gaussian noise :z\n",
    "    noise=tf.random.normal([batch_size,noise_dim])\n",
    "    with tf.GradientTape() as gen_tape,tf.GradientTape() as critic_tape:# x' = G(z)\n",
    "        fake_data=gen(noise,training=True)# s^ = c(x)\n",
    "        real_output=critic(real_data,training=True)# s_ = c(x')\n",
    "        fake_output=critic(fake_data,training=True)\n",
    "        critic_loss=tf.reduce_mean(fake_output)-tf.reduce_mean(real_output)\n",
    "        critic_loss_real=tf.reduce_mean(real_output)\n",
    "        critic_loss_fake=tf.reduce_mean(fake_output)# G loss fucntion is the critic's output for fake data -(s_)\n",
    "        gen_loss=-tf.reduce_mean(fake_output)\n",
    "    wasserstein=tf.reduce_mean(real_output)-tf.reduce_mean(fake_output)# calculate gradients for gen and critic to update them weights\n",
    "    gradients_of_generator=gen_tape.gradient(gen_loss,gen.trainable_variables)\n",
    "    gradients_of_critic=critic_tape.gradient(critic_loss,critic.trainable_variables)# update gen and critic weights \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator,gen.trainable_variables))\n",
    "    critic_optimizer.apply_gradients(zip(gradients_of_critic,critic.trainable_variables))\n",
    "    tf.group(*(var.assign(tf.clip_by_value(var,-0.01,0.01)) for var in critic.trainable_variables)) \n",
    "    return wasserstein,gen_loss,critic_loss_real,critic_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TiSClxsa42NV"
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_samples(generator,class_id,headers_name,nb_instance,NOISE_DIM):\n",
    "    # generete instances\n",
    "    fake_data=generator(tf.random.normal([nb_instance,NOISE_DIM]))\n",
    "    # prepare syhtentic dataset for export\n",
    "    synthetic_data=pd.DataFrame(data=np.array(fake_data),columns=headers_name)\n",
    "    synthetic_data[\"0\"]=np.repeat(class_id,len(fake_data))\n",
    "    # synthetic_data.to_csv(\"GAN_Synthetic_Data\"+str(class_id)+\".csv\",index=False,header=True)\n",
    "    return synthetic_data\n",
    "def fake_data_generation(training_data,nb_instances_to_generate,target):\n",
    "  # setting training parameters for GAN\n",
    "    BATCH_SIZE=8\n",
    "    NOISE_DIM=10\n",
    "    learning_rate=0.001\n",
    "    epochs=150# save column names for later\n",
    "    headers_name=list(training_data.columns.values)\n",
    "    headers_name=headers_name[0:-1]# prepre training data\n",
    "    # class_id=training_data[\"TypeGlass\"].values[0]\n",
    "    class_id=training_data[target].values[0]\n",
    "    print('CLASS ID',class_id)\n",
    "    X=training_data.iloc[:,:-1].values.astype(\"float32\")# number of features for training data \n",
    "    n_inp=X.shape[1]# slice training data into small batches\n",
    "    train_dataset=(tf.data.Dataset.from_tensor_slices(X.reshape(X.shape[0],n_inp)).batch(BATCH_SIZE))\n",
    "    # init the generator with number of features desired for the output and noise dimension\n",
    "    generator=Generator(n_inp,NOISE_DIM)\n",
    "    critic=Critic(n_inp)\n",
    "    # Init RMSprop optimizer for the generator and the critic \n",
    "    generator_optimizer=tf.keras.optimizers.RMSprop(learning_rate)\n",
    "    critic_optimizer=tf.keras.optimizers.RMSprop(learning_rate)\n",
    "    # WD distance across epochs\n",
    "    # Gen loss across epochs\n",
    "    # Desc loss across epochs\n",
    "    epoch_wasserstein=[] \n",
    "    epoch_gen_loss=[] \n",
    "    epoch_critic_loss_real=[] \n",
    "    epoch_critic_loss_fake=[]\n",
    "    for epoch in range(epochs):\n",
    "        batch_idx=0\n",
    "        batch_wasserstein=0\n",
    "        batch_gen=0\n",
    "        batch_critic_real=0\n",
    "        batch_critic_fake=0\n",
    "    # training\n",
    "        for batch in train_dataset:\n",
    "            wasserstein,gen_loss,critic_loss_real,critic_loss_fake=train_step(batch,generator,critic,NOISE_DIM,generator_optimizer,critic_optimizer)\n",
    "            epoch_wasserstein.append(wasserstein)\n",
    "            epoch_gen_loss.append(gen_loss)\n",
    "            epoch_critic_loss_real.append(critic_loss_real)\n",
    "            epoch_critic_loss_fake.append(critic_loss_fake)\n",
    "            batch_gen+=gen_loss\n",
    "            batch_critic_real+=critic_loss_real\n",
    "            batch_critic_fake+=critic_loss_fake\n",
    "            batch_wasserstein+=wasserstein\n",
    "            batch_idx+=1\n",
    "        batch_wasserstein=batch_wasserstein/batch_idx\n",
    "        batch_gen=batch_gen/batch_idx\n",
    "        batch_critic_real=batch_critic_real/batch_idx\n",
    "        batch_critic_fake=batch_critic_fake/batch_idx\n",
    "        if epoch%50==0:\n",
    "            print(\"Epoch %d / %d completed. Gen loss: %.8f. Desc loss_real: %.8f . Desc loss_fake: %.8f\"%(epoch+1,epochs,batch_gen,batch_critic_real,batch_critic_fake))\n",
    "            \"\"\"nb_instances_to_generate = len(class_0[\"target\"]) - len(class_1[\"target\"])    \"\"\"\n",
    "    data=generate_synthetic_samples(generator,class_id,headers_name,nb_instances_to_generate,NOISE_DIM)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cNFL8wXpMFeT"
   },
   "outputs": [],
   "source": [
    "# the function to generate fake data with WGAN for the given classes \n",
    "def gen_data(X_train,y_train,target,classes):\n",
    "    # count_classes=dict(y_train.value_counts())\n",
    "    count_classes=collections.Counter(y_train)\n",
    "    max_class=max(count_classes.values())\n",
    "    print('MAX CLASS',max_class)\n",
    "    new_data=pd.DataFrame()\n",
    "    tmp=X_train.copy()\n",
    "    tmp[target]=y_train\n",
    "    for c in set(classes):\n",
    "        training_data=tmp[tmp[target]==c]\n",
    "        nb_instances_to_generate=max_class-count_classes[c]\n",
    "        if nb_instances_to_generate !=0:\n",
    "            syhtnetic_data=fake_data_generation(training_data,nb_instances_to_generate,target)\n",
    "            syhtnetic_data.rename(columns={'0':target},inplace=True)\n",
    "            syhtnetic_data[target]=c\n",
    "            new_data=new_data.append(syhtnetic_data)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUl1nU2RBDJR"
   },
   "source": [
    "## TABGAN related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Gp6L7JIlpBaD"
   },
   "outputs": [],
   "source": [
    "def run_tabgan(X_train, y_train, X_test,y_test,target,classes):\n",
    "  count_classes=dict(y_train[target].value_counts())\n",
    "  max_class=max(count_classes.values())\n",
    "  new_data=pd.DataFrame()\n",
    "  new_train=pd.DataFrame()\n",
    "  new_target=pd.Series()\n",
    "  tmp=X_train.copy()\n",
    "  tmp[target]=y_train\n",
    "  tmp_test=X_test.copy()\n",
    "  tmp_test[target]=y_test\n",
    "  for c in set(classes):\n",
    "    training_data=tmp[tmp[target]==c]\n",
    "    print('CLASS',c)\n",
    "    nb_instances_to_generate=1+max_class/count_classes[c] \n",
    "    if nb_instances_to_generate !=1:\n",
    "      new_tr, new_tar = GANGenerator(gen_x_times=nb_instances_to_generate, cat_cols=None,\n",
    "            bot_filter_quantile=0.001, top_filter_quantile=0.999, is_post_process=False,\n",
    "            adversarial_model_params={\n",
    "                \"metrics\": \"AUC\", \"max_depth\": 2, \"max_bin\": 100, \n",
    "                \"learning_rate\": 0.02, \"random_state\": 42, \"n_estimators\": 500,\n",
    "            }, pregeneration_frac=2, only_generated_data=True,\n",
    "            gan_params = {\"batch_size\": 16, \"patience\": 5, \"epochs\" : 150,}).generate_data_pipe(pd.DataFrame(training_data.drop(target,1)), \n",
    "                                                                                                pd.DataFrame(training_data[target]), \n",
    "                                                                                                tmp_test[tmp_test[target]==c].drop(target,1), \n",
    "                                                                                                deep_copy=True, only_adversarial=False, \n",
    "                                                                                                use_adversarial=True)\n",
    "      new_train=new_train.append(new_tr)\n",
    "      new_target=new_target.append(new_tar)\n",
    "  new_target=pd.DataFrame(new_target,columns=[9])\n",
    "  return new_train, new_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLXvSBMDBI59"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VagMMEV6Lc3a"
   },
   "outputs": [],
   "source": [
    "# function to load data \n",
    "def data_loader(filename):\n",
    "    data=pd.read_csv(filename+\".csv\")\n",
    "    return data\n",
    "# check data on null values\n",
    "def check_notnull(data):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Number')\n",
    "    plt.title('Non-Missing Values in columns within %d instances ' % data.shape[0])\n",
    "    plt.bar(data.columns, data.notnull().sum())\n",
    "\n",
    "# functions for EDA\n",
    "def plot_displot(data):\n",
    "    fig = plt.figure(1, figsize=(20, 40))\n",
    "\n",
    "    for i in range(len(data.columns)):\n",
    "        fig.add_subplot(10, 5, i + 1)\n",
    "        sns.histplot(data.iloc[i], kde=True)\n",
    "        plt.axvline(data[data.columns[i]].mean(), c='green')\n",
    "        plt.axvline(data[data.columns[i]].median(), c='blue')\n",
    "\n",
    "def plot_scatter(data, x, y, target):\n",
    "    fig = plt.figure(1, figsize=(8, 5))\n",
    "    sns.scatterplot(data=data, x=x, y=y, hue=target)\n",
    "    plt.xlabel('ftr# {}'.format(x))\n",
    "    plt.ylabel('ftr# {}'.format(y))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_dist(target_column):\n",
    "    ax = target_column.value_counts().plot(kind='bar', figsize=(12, 8), \n",
    "                                           fontsize=12, \n",
    "                                           color=['#6ca5ce','#a06cce','#6cb4ce',\n",
    "                                                  '#6cce81','#c92c4c','#c726c9'])\n",
    "    ax.set_title('Target class\\n', size=20, pad=30)\n",
    "    ax.set_ylabel('Number of samples', fontsize=12)\n",
    "    for i in ax.patches:\n",
    "        ax.text(i.get_x() + 0.19, i.get_height(), str(round(i.get_height(), 2)), \n",
    "                fontsize=12)\n",
    "\n",
    "def plot_pie(data,labels,title):\n",
    "  #Usage:\n",
    "  # data = df[target].value_counts()\n",
    "  # print(df[target].value_counts(True)*100)\n",
    "  # plot_pie(data,classes,'Gallagher Dataset')\n",
    "    fig, ax = plt.subplots(figsize =(20, 10))\n",
    "    colors = sns.color_palette('pastel')\n",
    "    ax.pie(data, labels = labels, colors = colors)\n",
    "    ax.set_title(title,fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_dist(target_column):\n",
    "  # Usage: plot_class_dist(df[target])\n",
    "    ax = target_column.value_counts().plot(kind='bar', figsize=(12, 6),\n",
    "         fontsize=12, \n",
    "         color=['#6ca5ce','#a06cce','#6cb4ce','#6cce81','#c92c4c','#c726c9'])\n",
    "    ax.set_title('Target class\\n', size=16, pad=30)\n",
    "    ax.set_ylabel('Number of samples', fontsize=12)\n",
    "    for i in ax.patches:\n",
    "        ax.text(i.get_x() + 0.19, i.get_height(), str(round(i.get_height(), 2)),\n",
    "                fontsize=12)\n",
    "\n",
    "def fill_missing_values(data, num_features, cat_features):\n",
    "    for f in num_features:\n",
    "        median = data[f].mean()\n",
    "        data[f].fillna(median, inplace=True)\n",
    "    for col in cat_features:\n",
    "        most_frequent_category = data[col].mode()[0]\n",
    "        data[col].fillna(most_frequent_category, inplace=True)\n",
    "\n",
    "\n",
    "def encode_target(data, target):\n",
    "    label_encoder = LabelEncoder()\n",
    "    target_encoded = label_encoder.fit_transform(data[target])\n",
    "    return target_encoded\n",
    "\n",
    "\n",
    "def standardize_data(data, num_features):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data[num_features])\n",
    "    return scaled_data\n",
    "\n",
    "\n",
    "def transfrom_cat_features(data, cat_features):\n",
    "    for c in cat_features:\n",
    "        data = data.merge(pd.get_dummies(data[c], prefix=c), \n",
    "                          left_index=True, right_index=True)\n",
    "    data.drop(cat_features, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def split_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, \n",
    "                                                        stratify=target, \n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# get the oversampler model by key from dict\n",
    "def get_oversampler(oversamplers_dict, oversampler_num, proportion):\n",
    "    if proportion == None:\n",
    "        return oversamplers_dict[oversampler_num]()\n",
    "    else:\n",
    "        return oversamplers_dict[oversampler_num](proportion=proportion)\n",
    "\n",
    "def get_filter(filters_dict, filter_num):\n",
    "    return filters_dict[filter_num]\n",
    "\n",
    "# function to preprocess input data\n",
    "def preprocess(data, target, num_features, cat_features):\n",
    "    # check_notnull(data.drop(target, 1))\n",
    "    fill_missing_values(data.drop(target, 1), num_features, cat_features)\n",
    "    data[target] = encode_target(data, target)\n",
    "    data[num_features] = standardize_data(data, num_features)\n",
    "    transfrom_cat_features(data, cat_features)\n",
    "    return data\n",
    "\n",
    "# function to print evaluation metrics values\n",
    "def print_eval_results(y_test, preds):\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_test, preds))\n",
    "    print('Geometric mean:', geometric_mean_score(y_test, preds, \n",
    "                                                  average='weighted'))\n",
    "    print('Geometric mean default:', geometric_mean_score(y_test, preds))\n",
    "    print('Cohen Kappa', cohen_kappa_score(y_test, preds))\n",
    "\n",
    "# function to get optimal DT model\n",
    "def get_model(X_train, y_train):\n",
    "    param_grid = { 'criterion':['gini','entropy'],\n",
    "                  'max_depth': np.arange(3, 200),\n",
    "                  'max_features': ['auto', 'log2'],\n",
    "                  }\n",
    "    model=DecisionTreeClassifier()\n",
    "    adb = GridSearchCV(model, param_grid, cv=5,scoring='f1_weighted')\n",
    "    adb.fit(X_train, y_train)\n",
    "    return adb.best_estimator_ \n",
    "\n",
    "# function for calculation of error per class\n",
    "def error_per_class(y_test,preds,classes):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    # to store the results in a dictionary for easy access later\n",
    "    per_class_accuracies = {}\n",
    "    per_class_error={}\n",
    "    # Calculate the accuracy for each one of our classes\n",
    "    for idx, cls in enumerate(classes):\n",
    "        # TN - all the samples that are not current GT class \n",
    "        # and not predicted as the current class\n",
    "        true_negatives = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n",
    "        # TP are all the samples of current GT class that were predicted as such\n",
    "        true_positives = cm[idx, idx]\n",
    "        # accuracy for the current class\n",
    "        per_class_accuracies[cls] = (true_positives) / np.sum(cm[idx,:])\n",
    "        per_class_error[cls] = 1-(true_positives) / np.sum(cm[idx,:])\n",
    "    print('PER CLASS ERROR', per_class_error)\n",
    "    return per_class_error\n",
    "\n",
    "# functions to perform KS test for gen/real data\n",
    "def ks_test(real, gen):\n",
    "    df_a=np.array(real.values)\n",
    "    df_b=np.array(gen.values)\n",
    "    ks_scores=ks_2samp(df_a, df_b)\n",
    "    print(\"Gen vs Real: ks statistic\",ks_scores.statistic)\n",
    "    print(\"Gen vs Real: ks pvalue\",ks_scores.pvalue)\n",
    "    print(\"Gen & Real distributions are equal\",ks_scores.pvalue>0.05)\n",
    "\n",
    "def run_kstwo(X_sample,X_train):\n",
    "    df_gen=pd.DataFrame(X_sample.copy(),columns=X_train.columns)\n",
    "    df_gen=df_gen[~df_gen.isin(X_train)].dropna()\n",
    "    df_gen['gen']='generated'\n",
    "    df_real=pd.DataFrame(X_train.copy())\n",
    "    df_real['gen']='real'\n",
    "    df=pd.concat([df_real,df_gen])\n",
    "    for col in df_gen.drop(['gen'],1).columns.to_list():\n",
    "      print('Feature',col)\n",
    "      ks_test(df_gen[col], df_real[col])\n",
    "\n",
    "# functions for filtering data points \n",
    "\n",
    "# function to find N neighbors for a point\n",
    "def get_neighbours(X_train,X_gen):\n",
    "    nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree', p=2).fit(np.array(X_train.values))\n",
    "    _,ind=nbrs.kneighbors(np.array(X_gen.values))\n",
    "    return ind\n",
    "\n",
    "# filtering\n",
    "def filter_data(X_train,y_train,X_gen,y_gen,X_test,init_error,c,y_test,classes):\n",
    "    # init empty F set\n",
    "    n=len(X_gen)\n",
    "    X_filtered=pd.DataFrame()\n",
    "    y_filtered=pd.Series()\n",
    "    i=0\n",
    "    # find n-neighbors for the data point\n",
    "    k_neighbours=get_neighbours(X_train,X_gen)\n",
    "\n",
    "    for kn in k_neighbours:\n",
    "        X_tmp=X_train.copy()\n",
    "        y_tmp=y_train.copy()\n",
    "        # find class of the neigborhood \n",
    "        max_class=max(collections.Counter(y_train[kn]))\n",
    "        # if gen_sample class equals to neighborhood's class we append sample to F set\n",
    "        if max_class==y_gen.iloc[i]:\n",
    "            X_filtered=X_filtered.append(X_gen.iloc[i])\n",
    "            y_filtered=pd.concat([pd.Series(y_filtered),pd.Series(y_gen.iloc[i])])\n",
    "        # otherwise we check whether there is an improvement in error rate\n",
    "    else:\n",
    "        X_tmp=X_tmp.append(X_gen.iloc[i])\n",
    "        y_tmp=pd.concat([pd.Series(y_tmp),pd.Series(y_gen.iloc[i])])\n",
    "        clf_model=get_model(X_tmp,y_tmp)\n",
    "        preds = clf_model.predict(X_test)\n",
    "        error=error_per_class(y_test,preds,classes)\n",
    "        # if there is an improvement\n",
    "        # we append sample to F set\n",
    "        if init_error[c]>error[c]:\n",
    "            X_filtered.append(X_gen.iloc[i])\n",
    "            y_filtered.append(y_gen.iloc[i])\n",
    "    i+=1\n",
    "    return pd.DataFrame(X_filtered),pd.Series(y_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWiWYpoCBWJg"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ISkkOW-tdkMw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset shape (137, 9)\n",
      "Testset shape (68, 9)\n",
      "1    51\n",
      "0    47\n",
      "4    19\n",
      "2    11\n",
      "3     9\n",
      "Name: target, dtype: int64\n",
      "1    0.367647\n",
      "0    0.338235\n",
      "4    0.147059\n",
      "2    0.088235\n",
      "3    0.058824\n",
      "Name: target, dtype: float64\n",
      "WGAN OVERSAMPLING\n",
      "MAX CLASS 51\n",
      "CLASS ID 0\n",
      "Epoch 1 / 150 completed. Gen loss: -0.01971981. Desc loss_real: -0.01196079 . Desc loss_fake: 0.01971981\n",
      "Epoch 51 / 150 completed. Gen loss: 0.00168495. Desc loss_real: 0.00077757 . Desc loss_fake: -0.00168495\n",
      "Epoch 101 / 150 completed. Gen loss: 0.00058624. Desc loss_real: 0.00109291 . Desc loss_fake: -0.00058624\n",
      "         RI        Na        Mg        Al        Si         K        Ca  \\\n",
      "0  0.180595  0.202165  0.468156  0.169412  0.189298  0.212687  0.145131   \n",
      "1  0.230279  0.264411  0.535780  0.207683  0.250728  0.219904  0.238888   \n",
      "2  0.110280  0.189945  0.401584  0.127024  0.108957  0.111316  0.156176   \n",
      "3  0.095625  0.110677  0.658918  0.142931  0.085098  0.121127  0.112643   \n",
      "\n",
      "         Ba        Fe  target  \n",
      "0  0.110922  0.142493       0  \n",
      "1  0.207917  0.206117       0  \n",
      "2  0.092202  0.127878       0  \n",
      "3  0.060497  0.109015       0  \n",
      "         RI        Na        Mg        Al        Si         K        Ca  \\\n",
      "0  0.180595  0.202165  0.468156  0.169412  0.189298  0.212687  0.145131   \n",
      "1  0.230279  0.264411  0.535780  0.207683  0.250728  0.219904  0.238888   \n",
      "2  0.110280  0.189945  0.401584  0.127024  0.108957  0.111316  0.156176   \n",
      "3  0.095625  0.110677  0.658918  0.142931  0.085098  0.121127  0.112643   \n",
      "\n",
      "         Ba        Fe  target  \n",
      "0  0.110922  0.142493       0  \n",
      "1  0.207917  0.206117       0  \n",
      "2  0.092202  0.127878       0  \n",
      "3  0.060497  0.109015       0  \n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.60        23\n",
      "           1       0.78      0.56      0.65        25\n",
      "           2       0.27      0.50      0.35         6\n",
      "           3       0.50      1.00      0.67         4\n",
      "           4       1.00      0.70      0.82        10\n",
      "\n",
      "    accuracy                           0.62        68\n",
      "   macro avg       0.63      0.67      0.62        68\n",
      "weighted avg       0.68      0.62      0.63        68\n",
      "\n",
      "Geometric mean: 0.7353886388800573\n",
      "Geometric mean default: 0.6536289121277589\n",
      "Cohen Kappa 0.4881297046902142\n",
      "PER CLASS ERROR {0: 0.3913043478260869, 1: 0.43999999999999995}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # 1. Data Upload and Pre-processing \n",
    "    \n",
    "    ### START SECTION ###\n",
    "    ### PUT HERE STRINGS FROM README SECTION TO UPLOAD THE REQUIRED DATASET ###\n",
    "    filename = 'glass.csv'\n",
    "    target = 'target'\n",
    "    classes=[0, 1]\n",
    "    data = pd.read_csv(filename, header=0)\n",
    "    cat_features = []\n",
    "    num_features = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'] \n",
    "    \n",
    "    data = preprocess(data, target, num_features, cat_features)\n",
    "    X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), \n",
    "                                                  data[target])\n",
    "    X_train.reset_index(inplace=True,drop=True)\n",
    "    X_test.reset_index(inplace=True,drop=True)\n",
    "    y_train.reset_index(inplace=True,drop=True)\n",
    "    y_test.reset_index(inplace=True,drop=True)\n",
    "    ### END SECTION ###\n",
    "\n",
    "    print('Trainset shape', X_train.shape)\n",
    "    print('Testset shape', X_test.shape)\n",
    "    print(y_train.value_counts() )\n",
    "    print(y_test.value_counts() / y_test.shape[0])\n",
    "\n",
    "#     # 2. Baseline results\n",
    "\n",
    "#     print('BASELINE MODEL')\n",
    "#     clf_model=get_model(X_train,y_train)\n",
    "#     preds = clf_model.predict(X_test)\n",
    "#     print_eval_results(y_test, preds)\n",
    "#     #initial error_rate per class\n",
    "#     init_error=error_per_class(y_test,preds,classes)\n",
    "\n",
    "    # 3. WGAN oversampling \n",
    "    print('WGAN OVERSAMPLING')\n",
    "    X_sample=gen_data(X_train,y_train,target,classes)\n",
    "    print(X_sample)\n",
    "    X_sample.rename(columns={'TargetClass':target},inplace=True)\n",
    "    print(X_sample)\n",
    "    X_train[target]=y_train\n",
    "    X_sample=X_sample.append(X_train)\n",
    "    X_train=X_train.drop(target,1)\n",
    "    y_sample=X_sample[target]\n",
    "    X_sample=X_sample.drop(target,1)\n",
    "    clf_model=get_model(X_sample, y_sample)\n",
    "    preds = clf_model.predict(X_test)\n",
    "    print_eval_results(y_test, preds)\n",
    "    error_per_class(y_test,preds,classes)\n",
    "\n",
    "#     # 4. KS test for WGAN data\n",
    "#     print('KS tests for WGAN')\n",
    "#     run_kstwo(X_sample,X_train)\n",
    "\n",
    "#     # 5. SMOTE based oversampling \n",
    "#     print('SMOTE BASED OVERSAMPLING')\n",
    "#     oversamplers_dict = {1: sv.G_SMOTE, \n",
    "#                          2: sv.SMOTE, \n",
    "#                          3: sv.RWO_sampling, \n",
    "#                          5: sv.ANS, \n",
    "#                          6: sv.kmeans_SMOTE}\n",
    "#     oversampler = sv.MulticlassOversampling(get_oversampler(oversamplers_dict, 2, None))\n",
    "#     X_sample, y_sample = oversampler.sample(X_train.values, y_train.values)\n",
    "#     clf_model=get_model(X_sample, y_sample)\n",
    "#     preds = clf_model.predict(X_test)\n",
    "#     print_eval_results(y_test, preds)\n",
    "#     error_per_class(y_test,preds,classes)\n",
    "#     # 6. CTGAN based oversampling \n",
    "#     print('TABGAN OVERSAMPLING')\n",
    "#     X_sample,y_sample = run_tabgan(X_train, pd.DataFrame(y_train),X_test,\n",
    "#                                    pd.DataFrame(y_test),target,classes)\n",
    "#     X_sample=X_sample.append(pd.DataFrame(X_train))\n",
    "#     y_sample=pd.concat([pd.DataFrame(y_sample,columns=[9]),pd.DataFrame(y_train)])\n",
    "#     clf_model=get_model(X_sample,y_sample)\n",
    "#     preds = clf_model.predict(X_test)\n",
    "#     print_eval_results(y_test, preds)\n",
    "#     error_per_class(y_test,preds,classes)\n",
    "\n",
    "#     # 7. Filtering\n",
    "#     print('FILTERING PART')\n",
    "#     X_out=X_train.copy()\n",
    "#     y_out=y_train.copy()\n",
    "#     for c in classes:\n",
    "#         error=init_error\n",
    "#         ins=y_train.index[y_train==c].tolist()\n",
    "#         print('GEN DATA FOR CLASS ', c)\n",
    "#         X_gan= gen_data(X_train, pd.DataFrame(y_train),target,set([c])) #1\n",
    "#         y_gan = pd.Series(X_gan[target]) #2\n",
    "#         X_gan=X_gan.drop(target,1) #3\n",
    "#         #-----------OR-----------#\n",
    "#         # to test CTGAN replace 1,2,3 with\n",
    "#         # X_gan,y_gan = run_tabgan(X_train.iloc[ins], \n",
    "#         #                          pd.DataFrame(y_train).iloc[ins],\n",
    "#         #                          X_test,pd.DataFrame(y_test),target,set([c]))\n",
    "#         #-------------------------#\n",
    "#         X_filtered,y_filtered = filter_data(X_train,y_train,X_gan,pd.Series(y_gan),\n",
    "#                                           X_test,init_error,c,y_test,classes)\n",
    "#         X_out=X_out.append(pd.DataFrame(X_filtered))\n",
    "#         y_out=pd.concat([pd.DataFrame(y_out,columns=[target]),\n",
    "#                        pd.DataFrame(y_filtered,columns=[target])])\n",
    "#         y_out.reset_index(inplace=True,drop=True)\n",
    "#         X_out.reset_index(inplace=True,drop=True)\n",
    "#         X_gan=X_filtered.append(pd.DataFrame(X_train))\n",
    "#         y_gan=pd.concat([pd.DataFrame(y_filtered,columns=[target]),\n",
    "#                        pd.DataFrame(y_train)])\n",
    "#         clf_model=get_model(X_gan,y_gan.astype('int'))\n",
    "#         preds = clf_model.predict(X_test)\n",
    "#         error=error_per_class(y_test,preds,classes)\n",
    "#         y_gan.reset_index(inplace=True,drop=True)\n",
    "#         X_gan.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#     clf_model=get_model(X_out,y_out)\n",
    "#     preds = clf_model.predict(X_test)\n",
    "#     error=error_per_class(y_test,preds,classes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FIQsP50-x5T"
   },
   "source": [
    "## README "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUuMyMpt13EZ"
   },
   "source": [
    "\n",
    "\n",
    "### glass dataset <br>\n",
    "\n",
    "filename = 'glass.csv' <br>\n",
    "target = 'TypeGlass' <br>\n",
    "classes=[0,1,2,3,4,5] <br>\n",
    "data = pd.read_csv(filename, header=0) <br>\n",
    "cat_features = [] <br>\n",
    "num_features = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'] <br>\n",
    "data = preprocess(data, target, num_features, cat_features) <br>\n",
    "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target) <br>\n",
    "\n",
    "### dermathology dataset\n",
    "filename = 'dermatology.csv' <br>\n",
    "target = 'Class' <br>\n",
    "classes=[1,2,3,4,5,6] <br>\n",
    "data = pd.read_csv(filename, header=0) <br>\n",
    "cat_features = [] <br>\n",
    "num_features = ['Erythema', 'Scaling', 'Definite_borders', 'Itching',\n",
    "'Koebner_phenomenon', 'Polygonal_papules', 'Follicular_papules',\n",
    "'Oral_mucosal', 'Knee_and_elbow', 'Scalp_involvement', 'Family_history',\n",
    "'Melanin_incontinence', 'Eosinophils', 'PNL_infiltrate', 'Fibrosis',\n",
    "'Exocytosis', 'Acanthosis', 'Hyperkeratosis', 'Parakeratosis',\n",
    "'Clubbing', 'Elongation', 'Thinning', 'Spongiform_pustule',\n",
    "'Munro_microabcess', 'Focal_hypergranulosis', 'Granular_layer',\n",
    "'Vacuolisation', 'Spongiosis', 'Saw-tooth_appearance',\n",
    "'Follicular_horn_plug', 'Perifollicular_parakeratosis',\n",
    "'Inflammatory_monoluclear', 'Band-like_infiltrate', 'Age']  <br>\n",
    "data = preprocess(data, target, num_features, cat_features) <br>\n",
    "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target) <br>\n",
    "\n",
    "### wine dataset\n",
    "filename= 'wine.csv' <br>\n",
    "target = 'Class' <br>\n",
    "classes=[1,2,3] <br>\n",
    "data = pd.read_csv(filename, header=0) <br>\n",
    "cat_features = [] <br>\n",
    "num_features= ['Alcohol', 'MalicAcid', 'Ash', 'AlcalinityOfAsh', 'Magnesium',\n",
    "'TotalPhenols', 'flavanoids', 'NonflavanoidsPhenols', 'Proanthocyanins',\n",
    "'ColorIntensity', 'Hue', 'OD280/OD315', 'Proline'] <br>\n",
    "data = preprocess(data, target, num_features, cat_features) <br>\n",
    "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target) <br>\n",
    "\n",
    "### page-blocks\n",
    "filename = 'page-blocks.csv' <br>\n",
    "target = 'Class' <br>\n",
    "classes=[0,1,2,3,4,5,6] <br>\n",
    "data = pd.read_csv(filename, header=0) <br>\n",
    "cat_features = [] <br>\n",
    "num_features = ['Height', 'Lenght', 'Area', 'Eccen', 'P_black', 'P_and', 'Mean_tr','Blackpix', 'Blackand', 'Wb_trans'] <br>\n",
    "data = preprocess(data, target, num_features, cat_features) <br>\n",
    "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target]) <br>\n",
    "\n",
    "### internet firewall\n",
    "\n",
    "num_features=[\"Bytes Sent\",\"Bytes Received\",\"Elapsed Time (sec)\", \"Bytes\",'Packets',\"pkts_sent\",\"pkts_received\"] <br>\n",
    "cat_features = ['Source Port', 'Destination Port', 'NAT Source Port',\n",
    "       'NAT Destination Port'] <br>\n",
    "\n",
    "scaler = StandardScaler() <br>\n",
    "scaler.fit(data[num_features]) <br>\n",
    "data[num_features] = scaler.transform(data[num_features]) <br>\n",
    "data=data.astype({'Source Port': 'object',\n",
    "                  'Destination Port': 'object',\n",
    "                  'NAT Source Port': 'object',\n",
    "                  'NAT Destination Port': 'object',\n",
    "                  }) <br>\n",
    "count_enc = CountFrequencyEncoder(encoding_method=\"frequency\", variables=cat_features) <br>\n",
    "count_enc.fit(data) <br>\n",
    "data = count_enc.transform(data) <br>\n",
    "data.to_csv('log2_preproc.csv', index=False) <br>\n",
    "\n",
    "filename = 'log2_preproc.csv' <br>\n",
    "target = 'Action' <br>\n",
    "classes=[0,1,2,3] <br>\n",
    "data = pd.read_csv(filename, header=0) <br>\n",
    "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target]) <br>\n",
    "\n",
    "### shuttle dataset\n",
    "\n",
    "df_train=pd.read_csv('shuttle.trn', delimiter=' ',header=None) <br>\n",
    "df_test=pd.read_csv('shuttle.tst', delimiter=' ',header=None) <br>\n",
    "target=9 <br>\n",
    "num_features=[0,1,2,3,4,5,6,7,8] <br>\n",
    "cat_features=[] <br>\n",
    "df_train=preprocess(df_train, target, num_features, cat_features) <br>\n",
    "df_test=preprocess(df_test, target, num_features, cat_features) <br>\n",
    "X_train, y_train = df_train.drop(9,1),df_train[9] <br>\n",
    "X_test, y_test = df_test.drop(9,1),df_test[9] <br>\n",
    "\n",
    "### gallagher\n",
    "\n",
    "features_file = 'mobnet_features.npy' <br>\n",
    "vectors = np.load(features_file) <br>\n",
    "labels= np.load('mobnet_labels.npy') <br>\n",
    "label_enc = preprocessing.LabelEncoder() <br>\n",
    "label_enc.fit(labels) <br>\n",
    "labels = label_enc.transform(labels) <br>\n",
    "df=pd.DataFrame(vectors) <br>\n",
    "df['Class']=labels <br>\n",
    "target='Class' <br>\n",
    "classes=[11,22,0,30,31,27,26,28,3,17,29,13,6,25,10,19] <br>\n",
    "df=df[df.Class.isin(classes)] <br>\n",
    "df.reset_index(inplace=True,drop=True) <br>\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(target,1), df[target], test_size=0.33, random_state=42) <br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ImbLearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
